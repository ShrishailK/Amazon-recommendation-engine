{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6ad675",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb035e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary pacakages\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy.sparse\n",
    "from numpy import savez_compressed\n",
    "from numpy import load\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4a536",
   "metadata": {},
   "source": [
    "# Result visualization for IDF,TF-IDF, Bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087f6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function for results \n",
    "def display_img(url):\n",
    "    #Get url of the product and download it\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)\n",
    "\n",
    "def heatmap_image(keys,values,labels,url,text):\n",
    "    #keys gives the list of words for recommended title\n",
    "    #divide the figure into two parts\n",
    "    \n",
    "    gs = gridspec.GridSpec(1,2,width_ratios = [4,1])\n",
    "    fg = plt.figure(figsize=(25,3))\n",
    "    \n",
    "    #1st figure plotting a heatmap that represents the most commonly occuring words\n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax = sns.heatmap(np.array([values]),annot=np.array([labels]))\n",
    "    ax.set_xticklabels(keys)\n",
    "    ax.set_title(text)                 \n",
    "    \n",
    "    #2nd figure plotting a heatmap that represents the image of the product\n",
    "    ln = plt.subplot(gs[1])\n",
    "    ln.set_xticks([])\n",
    "    ln.set_yticks([])\n",
    "    \n",
    "    fig = display_img(url)\n",
    "    \n",
    "    #display combine figure\n",
    "    plt.show()\n",
    "\n",
    "def heatmap_image_plot(doc_id,vec1,vec2,url,text,model,tfidf_title_vectorizer,tfidf_title_features,idf_title_vectorizer,idf_title_features):\n",
    "    \n",
    "                     \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    \n",
    "    #set the value of non intersecting word to zero in vec2                 \n",
    "    for i in vec2.keys():\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "    #if ith word in intersection(list of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0                 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    \n",
    "    #labels for heatmap\n",
    "    keys = list(vec2.keys())\n",
    "                     \n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    \n",
    "    elif model == 'Tfidf':\n",
    "        labels = []\n",
    "        for i in vec2.keys():\n",
    "            if i in tfidf_title_vectorizer.vocabulary_:\n",
    "                #idf_title_vectorizer.vocabulary contains all the words in the corpus         \n",
    "                labels.append(tfidf_title_features[doc_id,tfidf_title_vectorizer.vocabulary_[i]])\n",
    "        \n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for i in vec2.keys():\n",
    "            if i in idf_title_vectorizer.vocabulary_:\n",
    "                #idf_title_vectorizer.vocabulary contains all the words in the corpus         \n",
    "                labels.append(idf_title_features[doc_id,idf_title_vectorizer.vocabulary_[i]])\n",
    "        \n",
    "            else:\n",
    "                labels.append(0)\n",
    "                     \n",
    "    heatmap_image(keys,values,labels,url,text)\n",
    "                     \n",
    "                     \n",
    "def text_vector(sentence):\n",
    "    words = sentence.split()    \n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def results(doc_id,sentence1,sentence2,url,model,tfidf_title_vectorizer,tfidf_title_features,idf_title_vectorizer,idf_title_features):\n",
    "    vec1 = text_vector(sentence1)\n",
    "    vec2 = text_vector(sentence2)\n",
    "                     \n",
    "    heatmap_image_plot(doc_id,vec1,vec2,url,sentence2,model,tfidf_title_vectorizer,tfidf_title_features,idf_title_vectorizer,idf_title_features)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fafb7",
   "metadata": {},
   "source": [
    "# Result visualization for Avg Word2Vec amd Weighted Word2Vec Vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8828040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uitlity function to better visualize and understand results\n",
    "\n",
    "def get_word_vec(sentence,doc_id,model_name,idf_title_vectorizer,idf_title_features):\n",
    "    #doc_id : index id in vectorized array\n",
    "    #sentence : title of product\n",
    "    #model_name : 'avg', we will append the model[i], w2v representation of word i\n",
    "    \n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "            if model_name == 'avg':\n",
    "                vec.append(modl[i])\n",
    "            elif model_name == 'weighted' and i in idf_title_vectorizer.vocabulary_:\n",
    "                vec.append(idf_title_features[doc_id,idf_title_vectorizer.vocabulary_[i]] * modl[i] )\n",
    "        else:\n",
    "            vec.append(np.zeros(shape=(300,)))\n",
    "    return np.array(vec)\n",
    "def get_distance(vec1,vec2):\n",
    "    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    final_dist = []\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "            \n",
    "    return np.array(final_dist)\n",
    "\n",
    "def results_Word2Vec(sentence1,sentence2,url,doc_id1,doc_id2,model_name,idf_title_vectorizer,idf_title_features):\n",
    "    # sentance1 : title1, input product\n",
    "    # sentance2 : title2, recommended product\n",
    "    # model:  'avg'\n",
    "\n",
    "    sentence_vec1 = get_word_vec(sentence1,doc_id1,model_name,idf_title_vectorizer,idf_title_features)\n",
    "    sentence_vec2 = get_word_vec(sentence2,doc_id2,model_name,idf_title_vectorizer,idf_title_features)\n",
    "    \n",
    "    #sent1_sent2_dist = eucledian distance between i and j\n",
    "    #sent1_sent2_dist = np array with dimensions(#number of words in title1 * #number of words in title2)\n",
    "    sent1_sent2_dist = get_distance(sentence_vec1,sentence_vec2)\n",
    "    \n",
    "    # devide whole figure into 2 parts 1st part displays heatmap 2nd part displays image of products\n",
    "    \n",
    "    gs = gridspec.GridSpec(1,2,width_ratios=[4,1])\n",
    "    fg = plt.figure(figsize=(25,25))\n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax = sns.heatmap(np.round(sent1_sent2_dist,3), annot = True)\n",
    "    ax.set_xticklabels(sentence2.split())\n",
    "    # set the y axis labels as input apparels title\n",
    "    ax.set_yticklabels(sentence1.split())\n",
    "    # set title as recommended apparels title\n",
    "    ax.set_title(sentence2)\n",
    "    \n",
    "    #setting the fontsize and rotation of x tick tables\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 12,rotation=90)\n",
    "    ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 12,rotation=45)\n",
    "    \n",
    "    fg = plt.subplot(gs[1])\n",
    "    fg.set_xticks([])\n",
    "    fg.set_yticks([])\n",
    "    fig = display_img(url)\n",
    "    \n",
    "    #display combine figure\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d950425",
   "metadata": {},
   "source": [
    "# Functions for Vectorization :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96226813",
   "metadata": {},
   "source": [
    "#### Function for IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23a7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for IDF\n",
    "def containing(word,df):\n",
    "    #returns the number of documents which have the word\n",
    "    return sum(1 for sentence in df['title'] if word in sentence.split())\n",
    "def idf(word,df):\n",
    "    #return the idf value for a word\n",
    "    return math.log(df.shape[0]/(containing(word,df)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3505bd7",
   "metadata": {},
   "source": [
    "####  Function for Word2Vec vectorization\n",
    "We perform Word2Vec vectorization in advance to use the vectorized array directly in distance based similarity recommendation as Word2Vec vectorization in computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c9d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_vec(sentence,no_features,id_,model_name,idf_title_vectorizer,idf_title_features):\n",
    "    \n",
    "    # sentence: title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # model_name: model information\n",
    "    # if  model_name == 'avg', we will add the value model[i], w2v representation of word i\n",
    "    # if mode_name ='weighted' we will add the value idf_title_features[doc_id,idf_title_vectorizer[word]] * model[word]\n",
    "    # idf_title_vectorizer : 0 for 'avg' and idf vectorized array for 'weighted'  \n",
    "    # idf_title_features : 0 for 'avg' and idf vectorized array for 'weighted'\n",
    "    \n",
    "    featureVec = np.zeros(shape=(300,), dtype=\"float32\")\n",
    "    # intialize a vector of size 300 with all zeros\n",
    "    # add each word2vec(wordi) to this fetureVec\n",
    "\n",
    "    ncount = 0\n",
    "    for word in sentence.split():\n",
    "        ncount += 1\n",
    "        if word in vocab:\n",
    "            if model_name == 'avg':\n",
    "                featureVec = np.add(featureVec,modl[word])\n",
    "            elif model_name == 'weighted' and word in idf_title_vectorizer.vocabulary_:\n",
    "                featureVec = np.add(featureVec, modl[word] * idf_title_features[id_,idf_title_vectorizer.vocabulary_[word]])\n",
    "        if (ncount>0):\n",
    "            featureVec = np.divide(featureVec,ncount)\n",
    "\n",
    "    #return avg vec\n",
    "    return featureVec    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533db58",
   "metadata": {},
   "source": [
    "#### Downloading the google Word2Vec library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc99485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Downloading Googles Word2Vec library to be used in all word to vec models\n",
    "# using a pretrained model by google\n",
    "# download \"GoogleNews-vectors-negative300.bin\" \n",
    "\n",
    "modl = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# vocab = stores all the words in google Word2vec model\n",
    "vocab = modl.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0f8bc",
   "metadata": {},
   "source": [
    "# Vectorization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c04fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorization(data,model):\n",
    "    #data : Data set containing text data\n",
    "    #model : method used for text vectorization\n",
    "    \n",
    "    if model == 'bag_of_words':\n",
    "        #Vectorization using Bag of words\n",
    "        title_vectorizer = CountVectorizer()\n",
    "        title_features = title_vectorizer.fit_transform(data['title'])\n",
    "        \n",
    "        return title_features,title_vectorizer\n",
    "    \n",
    "    elif model == 'Tfidf':\n",
    "        #Vectorization using tfidfVectorizer\n",
    "        tfidf_title_vectorizer = TfidfVectorizer()\n",
    "        tfidf_title_features = tfidf_title_vectorizer.fit_transform(data['title'])\n",
    "        \n",
    "        return tfidf_title_features,tfidf_title_vectorizer\n",
    "    \n",
    "    elif model == 'idf':\n",
    "        #Vectorization using idf function\n",
    "        idf_title_vectorizer = CountVectorizer()\n",
    "        idf_title_features = idf_title_vectorizer.fit_transform(data['title'])\n",
    "        # idf_title_features.shape = #no of_data_points * #words_corpus\n",
    "        # CountVectorizer().fit_transform(courpus) returns the a sparase matrix of dimensions #data_points * #words_in_corpus\n",
    "        \n",
    "        #converting all the values into float\n",
    "        idf_title_features = idf_title_features.astype(np.float)\n",
    "        \n",
    "        #assigning df value for idf[value] function\n",
    "        df = data\n",
    "        \n",
    "        for i in idf_title_vectorizer.vocabulary_.keys():\n",
    "            idf_value = idf(i,df)\n",
    "            #j is the index of the nonzero values\n",
    "            for j in idf_title_features[:,idf_title_vectorizer.vocabulary_[i]].nonzero()[0]:\n",
    "                idf_title_features[j,idf_title_vectorizer.vocabulary_[i]] = idf_value\n",
    "        \n",
    "        scipy.sparse.save_npz('Pickle/idf_title_features.npz', idf_title_features)\n",
    "        \n",
    "        return idf_title_features,idf_title_vectorizer\n",
    "    \n",
    "    elif model == 'avg':\n",
    "        w2vec_title_features = []\n",
    "        #building vector for each title \n",
    "        for i in data['title']:\n",
    "            w2vec_title_features.append(avg_word_vec(i,300,0,'avg',idf_title_vectorizer=0,idf_title_features=0))\n",
    "\n",
    "        #w2v_title_features = np.array(# number of doc/rows in courpus * 300) \n",
    "        Word2Vec_features = np.array(w2vec_title_features)\n",
    "        \n",
    "        #saving dataframe in a npz file\n",
    "        savez_compressed(\"Pickle/Word2Vec_aveg.npz\",Word2Vec_features)\n",
    "        \n",
    "        return w2vec_title_features\n",
    "    \n",
    "    elif model == 'weighted':\n",
    "        \n",
    "        #Load the saved idf vectorized sparse array .npz\n",
    "        #title_features= Vectorization(data,'idf')\n",
    "        idf_title_features = scipy.sparse.load_npz('Pickle/idf_title_features.npz') #OR we can Vectorize using the code above\n",
    "        \n",
    "        #to get the words in columns implemeny count vectorizers\n",
    "        idf_title_vectorizer = CountVectorizer()\n",
    "        vectorizer = idf_title_vectorizer.fit_transform(data['title'])\n",
    "        \n",
    "        id_ = 0 \n",
    "        w2vec_title_weight = []\n",
    "        \n",
    "        #building vector for each title\n",
    "        for i in data['title']:\n",
    "            w2vec_title_weight.append(avg_word_vec(i,300,id_,'weighted',idf_title_vectorizer = idf_title_vectorizer ,idf_title_features = idf_title_features))\n",
    "            id_ += 1\n",
    "\n",
    "        #w2v_title_weight = np.array(# number of doc/rows in courpus * 300) \n",
    "        w2vec_title_weight = np.array(w2vec_title_weight)\n",
    "        \n",
    "        #saving dataframe in a npz file\n",
    "        savez_compressed(\"Pickle/Word2Vec_weighted.npz\",w2vec_title_weight)\n",
    "        \n",
    "        return w2vec_title_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b859d",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fcaea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_similarity(doc_id,data,model,cut_off):\n",
    "    #data : data contaning text for vectorization \n",
    "    #model : method used for text vectorization\n",
    "    #Cut_off : the number of recommendations we give out\n",
    "    #Vector_array = loaded  Word2Vec vectorized numpy array from stored vectorization matrix saved in CSV file \n",
    "    \n",
    "    if model == 'bag_of_words': \n",
    "        #storing array after vectorization \n",
    "        title_features,title_vectorizer = Vectorization(data,model)\n",
    "\n",
    "        #doc_id is the number on the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distances saves the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(title_features,title_features[doc_id],metric = 'cosine')\n",
    "\n",
    "        #np.argsort returns indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index of the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results(indices[i], data['title'].loc[data_indices[0]], data['title'].loc[data_indices[i]], data['medium_image_url'].loc[data_indices[i]], model,tfidf_title_vectorizer=0,tfidf_title_features=0,idf_title_vectorizer=0,idf_title_features=0)\n",
    "            print('The amazon ID of the apparel is {}'.format(data['asin'].loc[data_indices[i]]))\n",
    "            \n",
    "    elif model == 'Tfidf':\n",
    "        #storing array after vectorization \n",
    "        tfidf_title_features,tfidf_title_vectorizer = Vectorization(data,model)\n",
    "\n",
    "        #doc_id is the number on the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distance saves the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(tfidf_title_features,tfidf_title_features[doc_id],metric = 'cosine')\n",
    "\n",
    "        #np.argsort returns indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index of the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results(indices[i], data['title'].loc[data_indices[0]], data['title'].loc[data_indices[i]], data['medium_image_url'].loc[data_indices[i]], model,tfidf_title_vectorizer=tfidf_title_vectorizer,tfidf_title_features=tfidf_title_features,idf_title_vectorizer=0,idf_title_features=0)\n",
    "            print('The amazon ID of the apparel is {}'.format(data['asin'].loc[data_indices[i]]))\n",
    "        \n",
    "    elif model == 'idf':\n",
    "        #do not use vectorizer as it is computationally expensive to vectorize everytime\n",
    "        #Load the saved vectorized sparse array .npz\n",
    "        #title_features= Vectorization(data,'idf')\n",
    "        idf_title_features = scipy.sparse.load_npz('Pickle/idf_title_features.npz') #OR we can Vectorize using the code above\n",
    "        \n",
    "        #to get the words in columns implemeny count vectorizers\n",
    "        idf_title_vectorizer = CountVectorizer()\n",
    "        vectorizer = idf_title_vectorizer.fit_transform(data['title'])\n",
    "\n",
    "        #doc_id is the number on the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distance will save the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(idf_title_features,idf_title_features[doc_id],metric = 'cosine')\n",
    "\n",
    "        #np.argsort will return indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index of the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results(indices[i], data['title'].loc[data_indices[0]], data['title'].loc[data_indices[i]], data['medium_image_url'].loc[data_indices[i]], model, tfidf_title_vectorizer=0, tfidf_title_features=0, idf_title_vectorizer = idf_title_vectorizer, idf_title_features = idf_title_features)\n",
    "            print('The amazon ID of the apparel is {}'.format(data['asin'].loc[data_indices[i]]))\n",
    "    \n",
    "    \n",
    "    elif model == 'avg':\n",
    "        #Word2Vec_features = Vectorization(data['title'],'avg')\n",
    "        #do not use vectorizer as it is computationally expensive to vectorize everytime \n",
    "        #Load the stored vectorized array .npz\n",
    "        Word2Vec_features = load(\"Pickle/Word2Vec_aveg.npz\") \n",
    "        \n",
    "        #uncompresing npz to numpy array array\n",
    "        Word2Vec_features  = Word2Vec_features['arr_0']\n",
    "        \n",
    "        #doc_id is the number on the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distance will save the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(Word2Vec_features,Word2Vec_features[doc_id].reshape(1,-1))\n",
    "\n",
    "        #np.argsort will return indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index of the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results_Word2Vec(data['title'].loc[data_indices[0]], data['title'].loc[data_indices[i]], data['medium_image_url'].loc[data_indices[i]], indices[0], indices[i],model,idf_title_vectorizer = 0,idf_title_features = 0)\n",
    "            print('The amazon ID of the apparel is {}'.format(data['asin'].loc[data_indices[i]]))\n",
    "    \n",
    "    elif model == 'weighted':\n",
    "        #do not use vectorizer as it is computationally expensive to vectorize everytime\n",
    "        #Load the saved vectorized sparse array .npz\n",
    "        #title_features= Vectorization(data,'weighted')\n",
    "        idf_title_features = scipy.sparse.load_npz('Pickle/idf_title_features.npz') #OR we can Vectorize using the code above\n",
    "        \n",
    "        #to get the words in columns CountVectorizer\n",
    "        idf_title_vectorizer = CountVectorizer()\n",
    "        vectorizer = idf_title_vectorizer.fit_transform(data['title'])\n",
    "        \n",
    "        #Word2Vec_features = Vectorization(data['title'],'avg')\n",
    "        #do not use vectorizer as it is computationally expensive to vectorize everytime \n",
    "        #Load the stored vectorized array .npz\n",
    "        Word2Vec_features = load(\"Pickle/Word2Vec_aveg.npz\") #OR we can Vectorize using the code above\n",
    "        \n",
    "        #uncompresing npz to numpy array array\n",
    "        Word2Vec_feature  = Word2Vec_features['arr_0']\n",
    "        \n",
    "        #doc_id is the number on the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distance will save the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances( Word2Vec_feature, Word2Vec_feature[doc_id].reshape(1,-1))\n",
    "\n",
    "        #np.argsort will return indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index of the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results_Word2Vec(data['title'].loc[data_indices[0]], data['title'].loc[data_indices[i]], data['medium_image_url'].loc[data_indices[i]], indices[0], indices[i],model,idf_title_vectorizer = idf_title_vectorizer,idf_title_features= idf_title_features)\n",
    "            print('The amazon ID of the apparel is {}'.format(data['asin'].loc[data_indices[i]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
