{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "befffcf6",
   "metadata": {},
   "source": [
    "# ETL Pipeline:\n",
    "\n",
    "Creating a preprocessing pipleine to extract the data, transform the data according to our solution needs and load the data into a data base to be used later.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92967544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a299e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL():\n",
    "    \n",
    "    #initializing the variables\n",
    "    def __init__(self,Amazon_filepath,Filepath):\n",
    "\n",
    "        #assigning csv file to Netflix_file variable \n",
    "        self.Amazon_filepath = Amazon_filepath\n",
    "\n",
    "        #assigning a address to save the cleaned data\n",
    "        self.Filepath = Filepath\n",
    "\n",
    "        #downloading Netflix datset\n",
    "        self.data = pd.read_json(self.Amazon_filepath)\n",
    "\n",
    "            \n",
    "    #Data loading\n",
    "    def load_data(self):\n",
    "\n",
    "        #printing the shape of our data set \n",
    "        print('The data has {} data points and {} features \\n'.format(self.data.shape[0],self.data.shape[1]))\n",
    "        #printing a space for presentation\n",
    "\n",
    "        #keeping just the pertinent features\n",
    "        self.data = self.data[['asin','product_type_name', 'formatted_price','title','medium_image_url']]\n",
    "\n",
    "        print('The data after removing irrelevant features has {} and it contains these {} features. The names of the features are {} \\n \\n'\n",
    "              .format(self.data.shape[0],self.data.shape[1],list(self.data.columns)))\n",
    "    \n",
    "    #Data analysis\n",
    "    def analysis(self):\n",
    "\n",
    "        #Basic stats for product type\n",
    "        print('The basic statistics for product type on amazon are as follows: \\n{}\\n\\n'\n",
    "              .format(self.data['product_type_name'].describe()))\n",
    "\n",
    "        #product type segregation\n",
    "        print('Product type count:\\n{}\\n\\n'.format(Counter(list(self.data['product_type_name'])).most_common(10)))\n",
    "\n",
    "        #basic stats for titles\n",
    "        print('The basic statistics for product type on amazon are as follows: \\n{}\\n\\n'.format(self.data['title'].describe()))\n",
    "\n",
    "        #Basic stats for product type\n",
    "        print('{} % of the total points have a listed price \\n \\n'\n",
    "              .format(self.data[~self.data['formatted_price'].isnull()].shape[0]/self.data.shape[0]*100))\n",
    "\n",
    "    #Data cleaning\n",
    "    def Data_cleaning(self):\n",
    "        \n",
    "        # removing products without a price as we need a price to sell products\n",
    "        self.data = self.data[~self.data['formatted_price'].isnull()]\n",
    "        print('The number of products (data points) remaining after removing products without a price: \\n{}\\n'\n",
    "              .format(self.data.shape[0]))\n",
    "\n",
    "        \n",
    "        #removing products without a title as we need titles for vectorization\n",
    "        #distance based similarity recommendation for title vectorization\n",
    "        self.data = self.data[~self.data['title'].isnull()]\n",
    "        print('The number of products (data points) remaining after removing products without a title description required for vectorization:\\n{}\\n'\n",
    "              .format(self.data.shape[0]))\n",
    "\n",
    "        \n",
    "        #removing products with small length titles as they might not adequately describe product\n",
    "        self.data = self.data[self.data['title'].apply(lambda x : len(x.split())>4)]\n",
    "        print('The number of products (data points) remaining after removing products with insufficient title descriptions required for vectorization:\\n{}\\n'\n",
    "              .format(self.data.shape[0]))\n",
    "         \n",
    "        #removing duplicate 'titles'\n",
    "        #Below is the code to remove similar titles with just 3 words differing from its duplicate \n",
    "        #################################################### start ###########################################################\n",
    "        indices = []\n",
    "        for i,row in self.data.iterrows():\n",
    "            indices.append(i)\n",
    "        \n",
    "        asins = []\n",
    "        while len(indices)!= 0:\n",
    "            i = indices.pop()\n",
    "\n",
    "            asins.append(self.data['asin'].loc[i])\n",
    "\n",
    "            a = self.data['title'].loc[i].split()\n",
    "        \n",
    "        # store the list of words of ith string in a lista = data['title'].loc[i].spilt()\n",
    "        for j in indices:\n",
    "            \n",
    "            # store the list of words of jth string in a list b = data['title'].loc[j].spilt()\n",
    "            b = self.data['title'].loc[j].split()\n",
    "            \n",
    "            #storing the max len between list a or b\n",
    "            length = max(len(a),len(b))\n",
    "            \n",
    "            # count is used to store the number of words that are matched in both lists\n",
    "            count = 0\n",
    "            \n",
    "            # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strng\n",
    "            # example: a =['a', 'b', 'c', 'd']\n",
    "            # b = ['a', 'b', 'd']\n",
    "            # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "            for h in itertools.zip_longest(a,b):\n",
    "                if (h[0]==h[1]):\n",
    "                    count += 1                    \n",
    "                \n",
    "            if (length - count) < 3:\n",
    "                indices.remove(j)            \n",
    "\n",
    "        #keeping product data points without a duplicate\n",
    "        self.data = self.data[self.data['asin'].isin(asins)]\n",
    "\n",
    "        print('The number of products (data points) remaining after removing products with duplicate title descriptions:\\n{}\\n\\n'\n",
    "              .format(self.data.shape[0]))\n",
    "        ####################################################################################################################            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #removing stopwords, terms which are not alphanumeric and lowering text\n",
    "        stopword = set(stopwords.words('english'))\n",
    "        \n",
    "        for index,rows in self.data.iterrows():\n",
    "            strng = \"\"\n",
    "            for words in rows['title'].split():\n",
    "\n",
    "                #removing special characters \n",
    "                word = (\"\".join(i for i in words if i.isalnum()))\n",
    "\n",
    "                #lowering the words\n",
    "                word = word.lower()\n",
    "                \n",
    "                #removing stopwords\n",
    "                if word not in stopword:\n",
    "                    strng += word + \" \"         \n",
    "            self.data['title'].loc[index] = strng\n",
    "            \n",
    "    def save_data(self):\n",
    "    \n",
    "        #saving data in a pickle file\n",
    "        self.data.to_pickle(self.Filepath)\n",
    "        \n",
    "    def processed(self):\n",
    "    \n",
    "        print('LOADING DATA...{}\\n\\n '.format(self.Amazon_filepath))\n",
    "        self.load_data()\n",
    "\n",
    "        print('DATA ANALYSIS...\\n\\n')\n",
    "        self.analysis()\n",
    "        \n",
    "        print('CLEANING DATA...\\n\\n')\n",
    "        self.Data_cleaning()\n",
    "        \n",
    "        print('SAVING DATA IN PICKLE FILE PREPROCESSED {}...\\n\\n'.format(self.Filepath))\n",
    "        self.save_data()\n",
    "        \n",
    "        print('Cleaned data saved to pickle file preprocessed in Pickle folder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
