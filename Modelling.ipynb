{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d6ad675",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb035e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary pacakages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import scipy.sparse\n",
    "from numpy import savez_compressed\n",
    "from numpy import load\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import math\n",
    "from ipynb.fs.full.Visualization import *\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "#Downloading Googles Word2Vec library to be used in all word to vec models using a pretrained model by google\n",
    "#download \"GoogleNews-vectors-negative300.bin\" \n",
    "modl = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#vocab = stores all the words in google Word2vec model\n",
    "vocab = modl.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d950425",
   "metadata": {},
   "source": [
    "# Functions for Vectorization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "01c9d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define additional functions needed for IDF vectorization\n",
    "def containing(word,df):\n",
    "    #returns the number of documents which have the word\n",
    "    return sum(1 for sentence in df['title'] if word in sentence.split())\n",
    "def idf(word,df):\n",
    "    #return the idf value for a word\n",
    "    return math.log(df.shape[0]/(containing(word,df)))        \n",
    "\n",
    "################################################################################################################################\n",
    "#define additional functions needed for avg and weighted Word2Vec vectorization\n",
    "#Function for Word2Vec vectorization\n",
    "#perform Word2Vec vectorization in advance to use the vectorized array directly in distance based similarity recommendation\n",
    "#as performing Word2Vec vectorization each time is computationally intensive compared to Bag of words and idf based vectorization.\n",
    "\n",
    "def avg_word_vec(sentence,no_features,id_,model_name,idf_title_vectorizer,idf_title_features):\n",
    "    \n",
    "    # sentence: title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # model_name: model information\n",
    "    # if  model_name == 'avg', add the value model[i], w2v representation of word i\n",
    "    # if mode_name ='weighted' add the value idf_title_features[doc_id,idf_title_vectorizer[word]] * model[word]\n",
    "    # idf_title_vectorizer : 0 for 'avg' and idf vectorized array for 'weighted'  \n",
    "    # idf_title_features : 0 for 'avg' and idf vectorized array for 'weighted'\n",
    "    \n",
    "    featureVec = np.zeros(shape=(300,), dtype=\"float32\")\n",
    "    # initialize a vector of size 300 with all zeros\n",
    "    # add each word2vec(wordi) to this fetureVec\n",
    "\n",
    "    ncount = 0\n",
    "    for word in sentence.split():\n",
    "        ncount += 1\n",
    "        if word in vocab:\n",
    "            if model_name == 'avg':\n",
    "                featureVec = np.add(featureVec,modl[word])\n",
    "            elif model_name == 'weighted' and word in idf_title_vectorizer.vocabulary_:\n",
    "                featureVec = np.add(featureVec, modl[word] * idf_title_features[id_,idf_title_vectorizer.vocabulary_[word]])\n",
    "        if (ncount>0):\n",
    "            featureVec = np.divide(featureVec,ncount)\n",
    "\n",
    "    #return avg vec\n",
    "    return featureVec    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533db58",
   "metadata": {},
   "source": [
    "# Class Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6c04fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class results():\n",
    "    \n",
    "    def __init__(self,doc_id,model,data,cut_off):        \n",
    "        \n",
    "        #initializing the movie for recommendation\n",
    "        self.doc_id = doc_id\n",
    "        \n",
    "        #initialzing the model to be used\n",
    "        self.model = model\n",
    "        \n",
    "        #initialzing the data to be modelled\n",
    "        self.data = data\n",
    "       \n",
    "        #the number of recommendations we require\n",
    "        self.cut_off = cut_off\n",
    "        \n",
    "    def Vectorization(self):\n",
    "        #data : Data set containing text data\n",
    "        #model : method used for text vectorization\n",
    "\n",
    "        if self.model == 'bag_of_words':\n",
    "            #Vectorization using Bag of words\n",
    "            title_vectorizer = CountVectorizer()\n",
    "            title_features = title_vectorizer.fit_transform(self.data['title'])   \n",
    "            return title_features,title_vectorizer\n",
    "\n",
    "        elif self.model == 'Tfidf':\n",
    "            #Vectorization using tfidfVectorizer\n",
    "            tfidf_title_vectorizer = TfidfVectorizer()\n",
    "            tfidf_title_features = tfidf_title_vectorizer.fit_transform(self.data['title'])\n",
    "            return tfidf_title_features,tfidf_title_vectorizer\n",
    "        \n",
    "        elif self.model == 'idf':\n",
    "            #Vectorization using idf function\n",
    "            idf_title_vectorizer = CountVectorizer()\n",
    "            idf_title_features = idf_title_vectorizer.fit_transform(self.data['title'])\n",
    "            \n",
    "            #converting all the values into float\n",
    "            idf_title_features = idf_title_features.astype(np.float)\n",
    "\n",
    "            #assigning df value for idf[value] function\n",
    "            df = self.data\n",
    "\n",
    "            for i in idf_title_vectorizer.vocabulary_.keys():\n",
    "                idf_value = idf(i,df)\n",
    "                #j is the index of the nonzero values\n",
    "                for j in idf_title_features[:,idf_title_vectorizer.vocabulary_[i]].nonzero()[0]:\n",
    "                    idf_title_features[j,idf_title_vectorizer.vocabulary_[i]] = idf_value\n",
    "        \n",
    "            scipy.sparse.save_npz('Pickle/idf_title_features.npz', idf_title_features)\n",
    "\n",
    "            return idf_title_features,idf_title_vectorizer\n",
    "        \n",
    "        elif self.model == 'avg':\n",
    "            w2vec_title_features = []\n",
    "            #building vector for each title \n",
    "            for i in self.data['title']:\n",
    "                w2vec_title_features.append(avg_word_vec(i,300))\n",
    "\n",
    "            #w2v_title_features = np.array(# number of doc/rows in courpus * 300) \n",
    "            Word2Vec_features = np.array(w2vec_title_features)\n",
    "\n",
    "            #saving dataframe in a npz file\n",
    "            savez_compressed(\"Pickle/Word2Vec_aveg.npz\",Word2Vec_features)\n",
    "            \n",
    "            return Word2Vec_features\n",
    "        \n",
    "        elif self.model == 'weighted':\n",
    "            #Load the saved idf vectorized sparse array .npz\n",
    "            #title_features= Vectorization(data,'idf')\n",
    "            idf_title_features = scipy.sparse.load_npz('Pickle/idf_title_features.npz') #OR we can Vectorize using the code above\n",
    "\n",
    "            #to get the words in columns implement count vectorizers\n",
    "            idf_title_vectorizer = CountVectorizer()\n",
    "            vectorizer = idf_title_vectorizer.fit_transform(data['title'])\n",
    "\n",
    "            id_ = 0 \n",
    "            w2vec_title_weight = []\n",
    "\n",
    "\n",
    "            #building vector for each title\n",
    "            for i in self.data['title']:\n",
    "                w2vec_title_weight.append(avg_word_vec(i,300,id_,'weighted',idf_title_vectorizer = idf_title_vectorizer ,idf_title_features = idf_title_features))\n",
    "                id_ += 1\n",
    "\n",
    "            #w2v_title_weight = np.array(# number of doc/rows in courpus * 300) \n",
    "            w2vec_title_weight = np.array(w2vec_title_weight)\n",
    "\n",
    "            #saving dataframe in a npz file\n",
    "            savez_compressed(\"Pickle/Word2Vec_weighted.npz\",w2vec_title_weight)\n",
    "\n",
    "            return w2vec_title_weight\n",
    "        \n",
    "        \n",
    "    def distance_similarity(self):\n",
    "        #data : data contaning text for vectorization \n",
    "        #model : method used for text vectorization\n",
    "        #Cut_off : the number of recommendations we give out\n",
    "        #df :  data set used to retrieve orignal movie description and genre\n",
    "        \n",
    "        if self.model == 'bag_of_words':  \n",
    "            title_features,title_vectorizer = self.Vectorization()\n",
    "\n",
    "            #doc_id is id on the new index formed after CountVectorizer is applied to the data['title']\n",
    "            #pairwise distances saves the distance between given input product and all other products\n",
    "            pairwise_dist = pairwise_distances(title_features,title_features[self.doc_id],metric = 'cosine')\n",
    "\n",
    "            #np.argsort returns indices of the smallest distances\n",
    "            indices = np.argsort(pairwise_dist.flatten())[:self.cut_off]\n",
    "\n",
    "            #get the index id of product in the original dataframe\n",
    "            data_indices = list(self.data.index[indices])\n",
    "            \n",
    "            for i in range(0,len(data_indices)):\n",
    "                visualization(indices[i], self.data['title'].loc[data_indices[0]], self.data['title'].loc[data_indices[i]], self.data['medium_image_url'].loc[data_indices[i]], 'bag_of_words',tfidf_title_vectorizer = 0,tfidf_title_features = 0, idf_title_vectorizer = 0,idf_title_features = 0)\n",
    "                print('The amazon ID of the apparel is {}'.format(self.data['asin'].loc[data_indices[i]]))\n",
    "\n",
    "        elif self.model == 'Tfidf':\n",
    "            #storing array after vectorization\n",
    "            tfidf_title_features,tfidf_title_vectorizer = self.Vectorization()\n",
    "\n",
    "            #doc_id is the id in the new index formed after CountVectorizer is applied to the data['title']\n",
    "            #pairwise distance saves the distance between given input product and all other products\n",
    "            pairwise_dist = pairwise_distances(tfidf_title_features,tfidf_title_features[self.doc_id],metric = 'cosine')\n",
    "\n",
    "            #np.argsort returns indices of the smallest distances\n",
    "            indices = np.argsort(pairwise_dist.flatten())[:self.cut_off]\n",
    "\n",
    "            #get the index id of product in the original dataframe\n",
    "            data_indices = list(self.data.index[indices])\n",
    "\n",
    "            for i in range(0,len(data_indices)):\n",
    "                visualization(indices[i], self.data['title'].loc[data_indices[0]], self.data['title'].loc[data_indices[i]], self.data['medium_image_url'].loc[data_indices[i]], 'Tfidf',tfidf_title_vectorizer,tfidf_title_features ,idf_title_vectorizer=0,idf_title_features=0)\n",
    "                print('The amazon ID of the apparel is {}'.format(self.data['asin'].loc[data_indices[i]]))\n",
    "                \n",
    "        elif self.model == 'idf':\n",
    "            #do not use vectorizer as it is computationally expensive to vectorize everytime\n",
    "            #Load the saved vectorized sparse array .npz\n",
    "            #title_features= Vectorization(data,'idf')\n",
    "            idf_title_features = scipy.sparse.load_npz('Pickle/idf_title_features.npz') #OR we can Vectorize using the code above\n",
    "            \n",
    "            idf_title_features =idf_title_features.toarray()\n",
    "            \n",
    "            #to get the words in columns implement count vectorizers\n",
    "            idf_title_vectorizer = CountVectorizer()\n",
    "            vectorizer = idf_title_vectorizer.fit_transform(self.data['title'])\n",
    "\n",
    "            #doc_id is the id in the new index formed after CountVectorizer is applied to the data['title']\n",
    "            #pairwise distance will save the distance between given input product and all other products\n",
    "            pairwise_dist = pairwise_distances(idf_title_features,idf_title_features[self.doc_id].reshape(1,-1),metric = 'cosine')\n",
    "\n",
    "            #np.argsort will return indices of the smallest distances\n",
    "            indices = np.argsort(pairwise_dist.flatten())[:self.cut_off]\n",
    "\n",
    "            #get the index id of product in the original dataframe\n",
    "            data_indices = list(self.data.index[indices])\n",
    "\n",
    "            for i in range(0,len(data_indices)):\n",
    "                visualization(indices[i], self.data['title'].loc[data_indices[0]], self.data['title'].loc[data_indices[i]], self.data['medium_image_url'].loc[data_indices[i]], 'idf', tfidf_title_vectorizer=0, tfidf_title_features=0, idf_title_vectorizer = idf_title_vectorizer, idf_title_features = idf_title_features)\n",
    "                print('The amazon ID of the apparel is {}'.format(self.data['asin'].loc[data_indices[i]]))\n",
    "        \n",
    "        elif self.model == 'avg':\n",
    "            #Word2Vec_features = Vectorization(data['title'],'avg')\n",
    "            #do not use vectorizer as it is computationally expensive to vectorize everytime \n",
    "            #Load the stored vectorized array .npz\n",
    "            Word2Vec_features = load(\"Pickle/Word2Vec_aveg.npz\")\n",
    "                      \n",
    "            #uncompresing npz to numpy array array\n",
    "            Word2Vec_features  = Word2Vec_features['arr_0']\n",
    "\n",
    "            #doc_id is the id of the product in the new index formed after CountVectorizer is applied to the data['title']\n",
    "            #pairwise distance will save the distance between given input product and all other products\n",
    "            pairwise_dist = pairwise_distances(Word2Vec_features,Word2Vec_features[self.doc_id].reshape(1,-1))\n",
    "\n",
    "            #np.argsort will return indices of the smallest distances\n",
    "            indices = np.argsort(pairwise_dist.flatten())[:self.cut_off]\n",
    "\n",
    "            #get the index id of product in the original dataframe\n",
    "            data_indices = list(self.data.index[indices])\n",
    "\n",
    "            for i in range(0,len(data_indices)):\n",
    "                results_Word2Vec(data['title'].loc[data_indices[0]], data['title'].loc[data_indices[i]], data['medium_image_url'].loc[data_indices[i]], indices[0], indices[i],'avg',idf_title_vectorizer = 0,idf_title_features = 0)\n",
    "                print('The amazon ID of the apparel is {}'.format(self.data['asin'].loc[data_indices[i]]))\n",
    "                      \n",
    "        elif self.model == 'weighted':\n",
    "            #do not use vectorizer as it is computationally expensive to vectorize everytime\n",
    "            #Load the saved vectorized sparse array .npz\n",
    "            #title_features= Vectorization(data,'weighted')\n",
    "            idf_title_features = scipy.sparse.load_npz('Pickle/idf_title_features.npz') #OR we can Vectorize using the code above\n",
    "            \n",
    "        \n",
    "            #to get the words in columns CountVectorizer\n",
    "            idf_title_vectorizer = CountVectorizer()\n",
    "            vectorizer = idf_title_vectorizer.fit_transform(self.data['title'])\n",
    "\n",
    "            #Word2Vec_features = Vectorization(data['title'],'avg')\n",
    "            #do not use vectorizer as it is computationally expensive to vectorize everytime \n",
    "            #Load the stored vectorized array .npz\n",
    "            Word2Vec_features = load(\"Pickle/Word2Vec_weighted.npz\") #OR we can Vectorize using the code above\n",
    "\n",
    "            #uncompresing npz to numpy array array\n",
    "            Word2Vec_feature  = Word2Vec_features['arr_0']\n",
    "\n",
    "            #doc_id is the id in the new index formed after CountVectorizer is applied to the data['title']\n",
    "            #pairwise distance will save the distance between given input product and all other products\n",
    "            pairwise_dist = pairwise_distances( Word2Vec_feature, Word2Vec_feature[self.doc_id].reshape(1,-1))\n",
    "\n",
    "            #np.argsort will return indices of the smallest distances\n",
    "            indices = np.argsort(pairwise_dist.flatten())[:self.cut_off]\n",
    "\n",
    "            #get the index of the original dataframe\n",
    "            data_indices = list(self.data.index[indices])\n",
    "\n",
    "            for i in range(0,len(data_indices)):\n",
    "                results_Word2Vec(self.data['title'].loc[data_indices[0]], self.data['title'].loc[data_indices[i]], data['medium_image_url'].loc[data_indices[i]], indices[0], indices[i],'weighted',idf_title_vectorizer,idf_title_features)\n",
    "                print('The amazon ID of the apparel is {}'.format(data['asin'].loc[self.data_indices[i]]))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
