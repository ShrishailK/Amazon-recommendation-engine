{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9077dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4a536",
   "metadata": {},
   "source": [
    "# Result visualization for IDF,TF-IDF, Bag of words vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "087f6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function for results \n",
    "def display_img(url):\n",
    "    #we get the url of the product and download it\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img)\n",
    "\n",
    "def heatmap_image(keys,values,labels,url,text):\n",
    "    #keys gives us the list of words for recommended title\n",
    "    #we will divide the figure into two parts\n",
    "    \n",
    "    #we will divide the figure into two parts\n",
    "    gs = gridspec.GridSpec(1,2,width_ratios = [4,1])\n",
    "    fg = plt.figure(figsize=(25,3))\n",
    "    \n",
    "    #1st figure plotting a heatmap that represents the most commonly occuring words\n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax = sns.heatmap(np.array([values]),annot=np.array([labels]))\n",
    "    ax.set_xticklabels(keys)\n",
    "    ax.set_title(text)                 \n",
    "    \n",
    "    #2nd figure plotting a heatmap that represents the most commonly occuring words\n",
    "    ln = plt.subplot(gs[1])\n",
    "    ln.set_xticks([])\n",
    "    ln.set_yticks([])\n",
    "    \n",
    "    fig = display_img(url)\n",
    "    \n",
    "    #display combine figure\n",
    "    plt.show()\n",
    "\n",
    "def heatmap_image_plot(doc_id,vec1,vec2,url,text,model):\n",
    "    \n",
    "                     \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    \n",
    "    #we set the value of non intersecting word to zero in vec2                 \n",
    "    for i in vec2.keys():\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "    #if ith word in intersection(list of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0                 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    \n",
    "    #labels for heatmap\n",
    "    keys = list(vec2.keys())\n",
    "                     \n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    \n",
    "    elif model == 'Tfidf':\n",
    "        labels = []\n",
    "        for i in vec2.keys():\n",
    "            if i in tfidf_title_vectorizer.vocabulary_:\n",
    "                #idf_title_vectorizer.vocabulary contains all the words in the corpus         \n",
    "                labels.append(tfidf_title_features[doc_id,tfidf_title_vectorizer.vocabulary_[i]])\n",
    "        \n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for i in vec2.keys():\n",
    "            if i in idf_title_vectorizer.vocabulary_:\n",
    "                #idf_title_vectorizer.vocabulary contains all the words in the corpus         \n",
    "                labels.append(idf_title_features[doc_id,idf_title_vectorizer.vocabulary_[i]])\n",
    "        \n",
    "            else:\n",
    "                labels.append(0)\n",
    "                     \n",
    "    heatmap_image(keys,values,labels,url,text)\n",
    "                     \n",
    "                     \n",
    "def text_vector(sentence):\n",
    "    words = sentence.split()    \n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def results(doc_id,sentence1,sentence2,url,model):\n",
    "    vec1 = text_vector(sentence1)\n",
    "    vec2 = text_vector(sentence2)\n",
    "                     \n",
    "    heatmap_image_plot(doc_id,vec1,vec2,url,sentence2,model)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fafb7",
   "metadata": {},
   "source": [
    "# Result visualization for Avg Word2Vec amd Weighted Word2Vec Vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8828040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uitlity function to better visualize and understand results\n",
    "\n",
    "def get_word_vec(sentence,doc_id,model_name):\n",
    "    #doc_id in our corpus\n",
    "    #sentence : title of product\n",
    "    #model_name : 'avg', we will append the model[i], w2v representation of word i\n",
    "    \n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "            if model_name == 'avg':\n",
    "                vec.append(model[i])\n",
    "            elif model_name == 'weighted' and i in idf_title_vectorizer.vocabulary_:\n",
    "                vec.append(idf_title_features[doc_id,idf_title_vectorizer.vocabulary_[i]] * model[i] )\n",
    "        else:\n",
    "            vec.append(np.zeros(shape=(300,)))\n",
    "    return np.array(vec)\n",
    "def get_distance(vec1,vec2):\n",
    "    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    final_dist = []\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "            \n",
    "    return np.array(final_dist)\n",
    "def results_Word2Vec(sentence1,sentence2,url,doc_id1,doc_id2,model_name):\n",
    "    # sentance1 : title1, input product\n",
    "    # sentance2 : title2, recommended product\n",
    "    # model:  'avg'\n",
    "\n",
    "    sentence_vec1 = get_word_vec(sentence1,doc_id1,model_name)\n",
    "    sentence_vec2 = get_word_vec(sentence2,doc_id2,model_name)\n",
    "    \n",
    "    #sent1_sent2_dist = eucledian distance between i and j\n",
    "    #sent1_sent2_dist = np array with dimensions(#number of words in title1 * #number of words in title2)\n",
    "    sent1_sent2_dist = get_distance(sentence_vec1,sentence_vec2)\n",
    "    \n",
    "    # devide whole figure into 2 parts 1st part displays heatmap 2nd part displays image of products\n",
    "    \n",
    "    gs = gridspec.GridSpec(1,2,width_ratios=[4,1])\n",
    "    fg = plt.figure(figsize=(35,25))\n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax = sns.heatmap(np.round(sent1_sent2_dist,), annot = True)\n",
    "    ax.set_xticklabels(sentence2.split())\n",
    "    # set the y axis labels as input apparels title\n",
    "    ax.set_yticklabels(sentence1.split())\n",
    "    # set title as recommended apparels title\n",
    "    ax.set_title(sentence2)\n",
    "    \n",
    "    #setting the fontsize and rotation of x tick tables\n",
    "    ax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 12,rotation=90)\n",
    "    ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 12,rotation=45)\n",
    "    \n",
    "    fg = plt.subplot(gs[1])\n",
    "    fg.set_xticks([])\n",
    "    fg.set_yticks([])\n",
    "    fig = display_img(url)\n",
    "    \n",
    "    #display combine figure\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d950425",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96226813",
   "metadata": {},
   "source": [
    "#### Function for IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23a7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions for IDF\n",
    "def containing(word):\n",
    "    #returns the number of documents which have the word\n",
    "    return sum(1 for sentence in data['title'] if word in sentence.split())\n",
    "def idf(word):\n",
    "    return math.log(data.shape[0] / (containing(word)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888a38e",
   "metadata": {},
   "source": [
    "#### Downloading Googles Word2Vec library to be used in all word to vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "498a5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are using a pretrained model by google\n",
    "# its 3.3G file, once you load this into your memory \n",
    "# it occupies ~9Gb, so please do this step only if you have >12G of ram\n",
    "# download \"GoogleNews-vectors-negative300.bin\" \n",
    "\n",
    "modl = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#word2vec vectorization of a sentence for both Avg Word2Vec and Weighted Word2Vec\n",
    "vocab = modl.index_to_key\n",
    "# vocab = stores all the words in google Word2vec model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3505bd7",
   "metadata": {},
   "source": [
    "###  Function for Word2Vec vectorization\n",
    "We perform Word2Vec vectorization in advance to use the vectorized array directly in distance based similarity recommendation as Word2Vec vectorization in computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c9d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_vec(sentence,no_features,id_,model_name):\n",
    "    \n",
    "    # sentace: its title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # model_name: model information it will take two values\n",
    "    # if  model_name == 'avg', we will add the value model[i], w2v representation of word i\n",
    "    # if mode_name ='weighted' we will add the value idf_title_features[doc_id,idf_title_vectorizer[word]] * model[word]\n",
    "    \n",
    "    featureVec = np.zeros(shape=(300,), dtype='float32')\n",
    "    # we will intialize a vector of size 300 with all zeros\n",
    "    # we add each word2vec(wordi) to this fetureVec\n",
    "\n",
    "    ncount = 0\n",
    "    for word in sentence.split():\n",
    "        ncount += 1\n",
    "        if word in vocab:\n",
    "            if model_name == 'avg':\n",
    "                np.add(featureVec,modl[word])\n",
    "            elif model_name == 'weighted' and word in idf_title_vectorizer.vocabulary_:\n",
    "                np.add(featureVec, modl[word] * idf_title_features[id_,idf_title_vectorizer.vocabulary_[word]])\n",
    "        if (ncount>0):\n",
    "            featureVec = np.divide(featureVec,ncount)\n",
    "\n",
    "    #return avg vec\n",
    "    return featureVec    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6c04fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vectorization(data,model):\n",
    "    \n",
    "    if model == 'bag_of_words':\n",
    "        #Vectorization using Bag of words\n",
    "        title_vectorizer = CountVectorizer()\n",
    "        title_features = title_vectorizer.fit_transform(data)\n",
    "        \n",
    "        return title_features,title_vectorizer\n",
    "    \n",
    "    elif model == 'Tfidf':\n",
    "        tfidf_title_vectorizer = TfidfVectorizer()\n",
    "        tfidf_title_features = tfidf_title_vectorizer.fit_transform(data)\n",
    "        \n",
    "        return tfidf_title_features,tfidf_title_vectorizer\n",
    "    \n",
    "    elif model == 'idf':\n",
    "        \n",
    "        idf_title_vectorizer = CountVectorizer()\n",
    "        idf_title_features = idf_title_vectorizer.fit_transform(data)\n",
    "        # idf_title_features.shape = #no of_data_points * #words_corpus\n",
    "        # CountVectorizer().fit_transform(courpus) returns the a sparase matrix of dimensions #data_points * #words_in_corpus\n",
    "        \n",
    "        #converting all the values into float\n",
    "        idf_title_features.astype(np.float)\n",
    "        \n",
    "        for i in idf_title_vectorizer.vocabulary_:\n",
    "            idf_value = idf(i)\n",
    "            \n",
    "            #j is the index of the nonzero values\n",
    "            for j in idf_title_features[:,idf_title_vectorizer.vocabulary_[i]].nonzero()[0]:\n",
    "                idf_title_features[j,idf_title_vectorizer.vocabulary_[i]]= idf_value\n",
    "                \n",
    "        return idf_title_features,idf_title_vectorizer\n",
    "    \n",
    "    elif model == 'avg':\n",
    "        id_ = 0\n",
    "        w2vec_title_features = []\n",
    "        #for every title we build a vector\n",
    "        for i in data['title']:\n",
    "            w2vec_title_features.append(avg_word_vec(i,300,id_,'avg'))\n",
    "            id_ += 1\n",
    "\n",
    "        #w2v_title_features = np.array(# number of doc/rows in courpus * 300) \n",
    "        w2vec_title_features = np.array(w2vec_title_features)\n",
    "        \n",
    "        return w2vec_title_features\n",
    "    \n",
    "    elif model == 'weighted':\n",
    "        id_ = 0 \n",
    "        w2vec_title_weight = []\n",
    "\n",
    "        #for every title we build a weighted vector representation\n",
    "        for i in data['title']:\n",
    "            w2vec_title_weight.append(avg_word_vec(i,300,id_,'weighted'))\n",
    "            id_ += 1\n",
    "\n",
    "        #w2v_title_weight = np.array(# number of doc/rows in courpus * 300) \n",
    "        w2vec_title_weight = np.array(w2vec_title_weight)\n",
    "    \n",
    "        return w2vec_title_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fcaea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_similarity(doc_id,data,model,cut_off):\n",
    "    \n",
    "    if model == 'bag_of_words' or 'Tfidf' or 'idf':\n",
    "        #storing array after vectorization \n",
    "        idf_title_features,idf_title_vectorizer = Vectorization(data['title'],model)\n",
    "\n",
    "        #doc_id is the number on the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distance will save the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(idf_title_features,idf_title_features[doc_id])\n",
    "\n",
    "        #np.argsort will return indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index of the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results(indices[i], data['title'].loc[data_indices[0]], data['title'].loc[data_indices[i]], data['medium_image_url'].loc[data_indices[i]], 'bag_of_words')\n",
    "            print('The amazon ID of the apparel is {}'.format(data['asin'].loc[data_indices[i]]))\n",
    "    \n",
    "    elif model == 'avg' or 'weighted':\n",
    "        \n",
    "        #title_features,title_vectorizer = Vectorization(data['title'],model)\n",
    "        #we do not use this as we save our vectorized array as it is computationally expensive to vectorize our entire title feature everytime\n",
    "        \n",
    "        #doc_id is the number on the new index formed after CountVectorizer is applied to the data['title']\n",
    "        #pairwise distance will save the distance between given input product and all other products\n",
    "        pairwise_dist = pairwise_distances(Word2Vec_features,Word2Vec_features[doc_id].reshape(1,-1))\n",
    "\n",
    "        #np.argsort will return indices of the smallest distances\n",
    "        indices = np.argsort(pairwise_dist.flatten())[:cut_off]\n",
    "\n",
    "        #get the index of the original dataframe\n",
    "        data_indices = list(data.index[indices])\n",
    "\n",
    "        for i in range(0,len(data_indices)):\n",
    "            results_Word2Vec(data['title'].loc[data_indices[0]],data['title'].loc[data_indices[i]],data['medium_image_url'].loc[data_indices[i]],indices[0],indices[i],'avg')\n",
    "            print('The amazon ID of the apparel is {}'.format(data['asin'].loc[data_indices[i]]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
