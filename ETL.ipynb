{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e72477",
   "metadata": {},
   "source": [
    "# ETL Pipeline:\n",
    "\n",
    "Creating a preprocessing pipleine to extract the data, transform the data according to our solution needs and load the data into a data base to be used later.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bf70dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8f7b1",
   "metadata": {},
   "source": [
    "# 1.Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9d057c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(Amazon_filepath):\n",
    "    \n",
    "    #importing amazon apparel dataset\n",
    "    data = pd.read_json(Amazon_filepath)\n",
    "    \n",
    "    #printing the shape of our data set \n",
    "    print('The data has {} data points and {} features \\n'.format(data.shape[0],data.shape[1]))\n",
    "    #printing a space for presentation\n",
    "    \n",
    "    \n",
    "    #keeping just the pertinent features\n",
    "    data = data[['asin','product_type_name', 'formatted_price','title','medium_image_url']]\n",
    "    \n",
    "    print('The data after removing irrelevant features has {} and it contains these {} features. The names of the features are {} \\n \\n'.format(data.shape[0],data.shape[1],list(data.columns)))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4b990",
   "metadata": {},
   "source": [
    "# 2.Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9faf0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(data):\n",
    "    \n",
    "    #Basic stats for product type\n",
    "    print('The basic statistics for product type on amazon are as follows: \\n{}\\n\\n'.format(data['product_type_name'].describe()))\n",
    "    \n",
    "    #product type segregation\n",
    "    print('Product type count:\\n{}\\n\\n'.format(Counter(list(data['product_type_name'])).most_common(10)))\n",
    "    \n",
    "    #basic stats for titles\n",
    "    print('The basic statistics for product type on amazon are as follows: \\n{}\\n\\n'.format(data['title'].describe()))\n",
    "    \n",
    "    #Basic stats for product type\n",
    "    print('{} % of the total points have a listed price \\n \\n'.format(data[~data['formatted_price'].isnull()].shape[0]/data.shape[0]*100))\n",
    "\n",
    "                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad13377",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a914f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stopwords\n",
    "def text_preprocessing(text,index,column,stopword):\n",
    "    \n",
    "    if type(text) is not int:\n",
    "        strng = \"\"\n",
    "        for words in text.split():\n",
    "            #removing special characters \n",
    "            word = (\"\".join(i for i in words if i.isalnum()))\n",
    "            \n",
    "            #lowering the words\n",
    "            word = word.lower()\n",
    "            \n",
    "            #removing stopwords\n",
    "            if word not in stopword:\n",
    "                strng += word + \" \" \n",
    "        data[column][index] = str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "62b7cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_cleaning(data):\n",
    "    # removing apparels without a price as we need a price to sell apparelss\n",
    "    data = data[~data['formatted_price'].isnull()]\n",
    "    print('The number of products (data points) remaining after removing products without a price: \\n{}\\n'.format(data.shape[0]))\n",
    "        \n",
    "    #removing appaerls without a title as we need titles for vectorization\n",
    "    #distance based similarity recommendation for title vectorization\n",
    "    data = data[~data['title'].isnull()]\n",
    "    print('The number of products (data points) remaining after removing products without a title description required for vectorization:\\n{}\\n'.format(data.shape[0]))\n",
    "    \n",
    "    #removing apparels with small length titles as they might not adequately describe apparel\n",
    "    data = data[data['title'].apply(lambda x : len(x.split())>4)]\n",
    "    print('The number of products (data points) remaining after removing products with insufficient title descriptions required for vectorization:\\n{}\\n'.format(data.shape[0]))\n",
    "   \n",
    "\n",
    "\n",
    "    #removing duplicate 'titles'\n",
    "    #Below is the code to remove similar titles with just 3 words differing from its duplicate \n",
    "    ##################### start ############################\n",
    "    indices = []\n",
    "    for i,row in data.iterrows():\n",
    "        indices.append(i)\n",
    "    \n",
    "    asins = []\n",
    "    while len(indices)!= 0:\n",
    "        i = indices.pop()\n",
    "    \n",
    "        asins.append(data['asin'].loc[i])\n",
    "        \n",
    "        a = data['title'].loc[i].split()\n",
    "        \n",
    "        # store the list of words of ith string in a lista = data['title'].loc[i].spilt()\n",
    "        for j in indices:\n",
    "            \n",
    "            # store the list of words of jth string in a list b = data['title'].loc[j].spilt()\n",
    "            b = data['title'].loc[j].split()\n",
    "            \n",
    "            #storing the max len between list a or b\n",
    "            length = max(len(a),len(b))\n",
    "            \n",
    "            # count is used to store the number of words that are matched in both lists\n",
    "            count = 0\n",
    "            \n",
    "            # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings\n",
    "            # example: a =['a', 'b', 'c', 'd']\n",
    "            # b = ['a', 'b', 'd']\n",
    "            # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "            for h in itertools.zip_longest(a,b):\n",
    "                if (h[0]==h[1]):\n",
    "                    count += 1                    \n",
    "                \n",
    "            if (length - count) < 3:\n",
    "                indices.remove(j)            \n",
    "                \n",
    "    #keeping apparel data points without a duplicate\n",
    "    data = data[data['asin'].isin(asins)]\n",
    "    \n",
    "    print('The number of products (data points) remaining after removing products with duplicate title descriptions:\\n{}\\n\\n'.format(data.shape[0]))\n",
    "    ################### end #####################\n",
    "    \n",
    "    \n",
    "    #Removing stopwords from title description\n",
    "    #dictionary containing all the stopwords\n",
    "    stopword = set(stopwords.words('english'))\n",
    "        \n",
    "    for index,rows in data.iterrows():\n",
    "        text_preprocessing(row['title'],index,'title',stopword)\n",
    "    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a4214",
   "metadata": {},
   "source": [
    "# 4.Data Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9a145a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data,Filepath):\n",
    "    \n",
    "    #saving data in a pickle file\n",
    "    data.to_pickle(Filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b1bef",
   "metadata": {},
   "source": [
    "# 5.Main Excecution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d237d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(Amazon_filepath,Filepath):\n",
    "    \n",
    "        print('LOADING DATA...{}\\n\\n '.format(Amazon_filepath))\n",
    "        data = load_data(Amazon_filepath)\n",
    "\n",
    "        print('DATA ANALYSIS...\\n\\n')\n",
    "        analysis(data)\n",
    "        \n",
    "        print('CLEANING DATA...\\n\\n')\n",
    "        data = Data_cleaning(data)\n",
    "        \n",
    "        print('SAVING DATA IN PICKLE FILE PREPROCESSED {}...\\n\\n'.format(Filepath))\n",
    "        save_data(data,Filepath)\n",
    "        \n",
    "        print('Cleaned data saved to pickle file preprocessed in Pickle folder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
